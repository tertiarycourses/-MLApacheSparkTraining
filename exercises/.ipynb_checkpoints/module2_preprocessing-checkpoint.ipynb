{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('abc').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = spark.createDataFrame([\n",
    "(1,Vectors.dense([10.0,10000.0,1.0]),),\n",
    "(2,Vectors.dense([20.0,20000.0,2.0]),),\n",
    "(3,Vectors.dense([30.0,30000.0,3.0]),)\n",
    "],[\"id\",\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|          features|\n",
      "+---+------------------+\n",
      "|  1|[10.0,10000.0,1.0]|\n",
      "|  2|[20.0,20000.0,2.0]|\n",
      "|  3|[30.0,30000.0,3.0]|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"sfeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = scaler.fit(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = model.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+\n",
      "|          features|    sfeatures|\n",
      "+------------------+-------------+\n",
      "|[10.0,10000.0,1.0]|[0.0,0.0,0.0]|\n",
      "|[20.0,20000.0,2.0]|[0.5,0.5,0.5]|\n",
      "|[30.0,30000.0,3.0]|[1.0,1.0,1.0]|\n",
      "+------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(\"features\",\"sfeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler(inputCol=\"features\",outputCol=\"sfeatures\",withStd=True,withMean=True)\n",
    "\n",
    "model2 = scaler2.fit(df4)\n",
    "\n",
    "df6 = model2.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler(inputCol=\"features\",outputCol=\"sfeatures\",withStd=True,withMean=True)\n",
    "model2 = scaler2.fit(df4)\n",
    "df6 = model2.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|          features|       sfeatures|\n",
      "+------------------+----------------+\n",
      "|[10.0,10000.0,1.0]|[-1.0,-1.0,-1.0]|\n",
      "|[20.0,20000.0,2.0]|   [0.0,0.0,0.0]|\n",
      "|[30.0,30000.0,3.0]|   [1.0,1.0,1.0]|\n",
      "+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.select(\"features\",\"sfeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cctv = spark.read.csv('./data/cctv.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+\n",
      "|   AT|    V|     AP|   RH|    PE|\n",
      "+-----+-----+-------+-----+------+\n",
      "| 8.34|40.77|1010.84|90.01|480.48|\n",
      "|23.64|58.49| 1011.4| 74.2|445.75|\n",
      "|29.74| 56.9|1007.15|41.91|438.76|\n",
      "|19.07|49.69|1007.22|76.79|453.09|\n",
      "| 11.8|40.66|1017.13| 97.2|464.43|\n",
      "|13.97|39.16|1016.05| 84.6|470.96|\n",
      "| 22.1|71.29| 1008.2|75.38|442.35|\n",
      "|14.47|41.76|1021.98|78.41| 464.0|\n",
      "|31.25|69.51|1010.25|36.83|428.77|\n",
      "| 6.77|38.18| 1017.8|81.13|484.31|\n",
      "|28.28|68.67|1006.36| 69.9|435.29|\n",
      "|22.99|46.93|1014.15|49.42|451.41|\n",
      "| 29.3|70.04|1010.95|61.23|426.25|\n",
      "| 8.14|37.49|1009.04|80.33|480.66|\n",
      "|16.92| 44.6|1017.34|58.75|460.17|\n",
      "|22.72|64.15|1021.14|60.34|453.13|\n",
      "|18.14|43.56|1012.83| 47.1|461.71|\n",
      "|11.49|44.63|1020.44|86.04|471.08|\n",
      "| 9.94|40.46| 1018.9|68.51|473.74|\n",
      "|23.54| 41.1|1002.05|38.05|448.56|\n",
      "+-----+-----+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cctv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"AT\",\"V\",\"AP\",\"RH\"],outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cctv2 = vectorAssembler.transform(cctv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+---------------------------+\n",
      "|AT   |V    |AP     |RH   |PE    |features                   |\n",
      "+-----+-----+-------+-----+------+---------------------------+\n",
      "|8.34 |40.77|1010.84|90.01|480.48|[8.34,40.77,1010.84,90.01] |\n",
      "|23.64|58.49|1011.4 |74.2 |445.75|[23.64,58.49,1011.4,74.2]  |\n",
      "|29.74|56.9 |1007.15|41.91|438.76|[29.74,56.9,1007.15,41.91] |\n",
      "|19.07|49.69|1007.22|76.79|453.09|[19.07,49.69,1007.22,76.79]|\n",
      "|11.8 |40.66|1017.13|97.2 |464.43|[11.8,40.66,1017.13,97.2]  |\n",
      "|13.97|39.16|1016.05|84.6 |470.96|[13.97,39.16,1016.05,84.6] |\n",
      "|22.1 |71.29|1008.2 |75.38|442.35|[22.1,71.29,1008.2,75.38]  |\n",
      "|14.47|41.76|1021.98|78.41|464.0 |[14.47,41.76,1021.98,78.41]|\n",
      "|31.25|69.51|1010.25|36.83|428.77|[31.25,69.51,1010.25,36.83]|\n",
      "|6.77 |38.18|1017.8 |81.13|484.31|[6.77,38.18,1017.8,81.13]  |\n",
      "|28.28|68.67|1006.36|69.9 |435.29|[28.28,68.67,1006.36,69.9] |\n",
      "|22.99|46.93|1014.15|49.42|451.41|[22.99,46.93,1014.15,49.42]|\n",
      "|29.3 |70.04|1010.95|61.23|426.25|[29.3,70.04,1010.95,61.23] |\n",
      "|8.14 |37.49|1009.04|80.33|480.66|[8.14,37.49,1009.04,80.33] |\n",
      "|16.92|44.6 |1017.34|58.75|460.17|[16.92,44.6,1017.34,58.75] |\n",
      "|22.72|64.15|1021.14|60.34|453.13|[22.72,64.15,1021.14,60.34]|\n",
      "|18.14|43.56|1012.83|47.1 |461.71|[18.14,43.56,1012.83,47.1] |\n",
      "|11.49|44.63|1020.44|86.04|471.08|[11.49,44.63,1020.44,86.04]|\n",
      "|9.94 |40.46|1018.9 |68.51|473.74|[9.94,40.46,1018.9,68.51]  |\n",
      "|23.54|41.1 |1002.05|38.05|448.56|[23.54,41.1,1002.05,38.05] |\n",
      "+-----+-----+-------+-----+------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cctv2.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"sfeatures\")\n",
    "model = scaler.fit(cctv2)\n",
    "cctv3 = model.transform(cctv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|sfeatures                                                                      |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[0.184985835694051,0.2741992882562278,0.444196980945312,0.8639410187667561]    |\n",
      "|[0.6184135977337112,0.5895017793594306,0.4580549368968079,0.6520107238605899]  |\n",
      "|[0.7912181303116148,0.5612099644128113,0.35288294976490975,0.21916890080428952]|\n",
      "|[0.48895184135977343,0.4329181494661921,0.35461519425884813,0.6867292225201074]|\n",
      "|[0.2830028328611898,0.27224199288256223,0.5998515219005204,0.9603217158176944] |\n",
      "|[0.3444759206798867,0.24555160142348748,0.5731254639940605,0.7914209115281501] |\n",
      "|[0.574787535410765,0.8172597864768684,0.3788666171739686,0.6678284182305629]   |\n",
      "|[0.3586402266288952,0.29181494661921703,0.7198713189804518,0.7084450402144772] |\n",
      "|[0.8339943342776205,0.7855871886120998,0.42959663449641244,0.15107238605898124]|\n",
      "|[0.1405099150141643,0.2281138790035587,0.6164315763424891,0.7449061662198391]  |\n",
      "|[0.7498583569405101,0.7706405693950178,0.33333333333333426,0.5943699731903486] |\n",
      "|[0.6000000000000001,0.38380782918149464,0.5261073991586243,0.3198391420911529] |\n",
      "|[0.7787535410764874,0.7950177935943061,0.446919079435785,0.47815013404825746]  |\n",
      "|[0.1793201133144476,0.21583629893238437,0.3996535511012123,0.7341823056300268] |\n",
      "|[0.4280453257790369,0.3423487544483986,0.6050482553823328,0.44490616621983914] |\n",
      "|[0.5923512747875355,0.6902135231316727,0.6990843850532052,0.4662198391420912]  |\n",
      "|[0.46260623229461767,0.3238434163701068,0.49344221727295395,0.2887399463806971]|\n",
      "|[0.27422096317280453,0.3428825622775801,0.6817619401138354,0.8107238605898125] |\n",
      "|[0.2303116147308782,0.26868327402135234,0.6436525612472164,0.5757372654155497] |\n",
      "|[0.6155807365439094,0.28007117437722423,0.2266765652066314,0.16742627345844502]|\n",
      "+-------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cctv3.select('sfeatures').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, spark i j k) --> prob=[0.1596407738787475,0.8403592261212525], prediction=1.000000\n",
      "(5, l m n) --> prob=[0.8378325685476744,0.16216743145232562], prediction=0.000000\n",
      "(6, spark hadoop spark) --> prob=[0.06926633132976037,0.9307336686702395], prediction=1.000000\n",
      "(7, apache hadoop) --> prob=[0.9821575333444218,0.01784246665557808], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "df7 = spark.createDataFrame([\n",
    "(1,\"This is an introduction to Spark MLib\"),\n",
    "(2,\"MLib contains libraries for classification and regression\"),\n",
    "(3,\"It also contains supporting tools for pipelines\")\n",
    "],[\"id\",\"sentence\"])\n",
    "\n",
    "df7.show()\n",
    "\n",
    "token = Tokenizer(inputCol=\"sentence\",outputCol=\"words\")\n",
    "df8 = token.transform(df7)\n",
    "\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hftoken = HashingTF(inputCol=\"words\",outputCol=\"rawFeatures\",numFeatures=20)\n",
    "df9 = hftoken.transform(df8)\n",
    "\n",
    "df9.take(1)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\",outputCol=\"idf_features\")\n",
    "model3 = idf.fit(df9)\n",
    "df10 = model3.transform(df9)\n",
    "\n",
    "df10.take(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
